{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d985ae-e1f4-443e-8ac5-18b479d15587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b4d226-41d0-46ad-be84-dca0567bc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Melbourne_housing_FULL.txt')\n",
    "# df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf794118",
   "metadata": {},
   "source": [
    "### Dataset columns:\n",
    "\n",
    "- **suburb** (Suburb): The suburb where the property is located\n",
    "- **address** (Address): The street address of the property\n",
    "- **rooms** (Rooms): The number of rooms in the property\n",
    "- **property_type** (Type): The type of the property (house, townhouse, unit, etc.)\n",
    "- **price** (Price): The price at which the property was sold\n",
    "- **sale_type** (Method): The method of sale (auction, private treaty, etc.)\n",
    "- **seller** (SellerG): The agency or agent who sold the property\n",
    "- **date** (Date): The date on which the property was sold\n",
    "- **distance** (Distance): The distance of the property from Melbourne's central business district (CBD) in kilometers\n",
    "- **postcode** (Postcode): The postcode of the suburb where the property is located\n",
    "- **bedrooms** (Bedroom2): The number of bedrooms in the property (other than the master bedroom)\n",
    "- **bathroom** (Bathroom): The number of bathrooms in the property\n",
    "- **car_spaces** (Car): The number of car spaces in the property\n",
    "- **land_size** (Landsize): The size of the land on which the property is located in square meters\n",
    "- **building_area** (BuildingArea): The size of the building on the land in square meters\n",
    "- **building_year** (YearBuilt): The year in which the building was constructed\n",
    "- **council** (CouncilArea): The local government area in which the property is located\n",
    "- **latitude** (Latitude): The latitude coordinate of the property\n",
    "- **longitude** (Longitude): The longitude coordinate of the property\n",
    "- **region** (Regionname): The general region in Melbourne (west, north, south-east, etc.)\n",
    "- **property_count** (Propertycount): The number of properties in the suburb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd2f5f",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91881bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDF basic info:')\n",
    "print(df.info())\n",
    "\n",
    "print('\\nSummary stats:')\n",
    "print(df.describe())\n",
    "\n",
    "print('\\nNull values:')\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names\n",
    "df.columns = [\n",
    "    'suburb', 'address', 'rooms', 'property_type', 'price', 'sale_type', 'seller', \n",
    "    'date', 'distance', 'postcode', 'bedrooms', 'bathroom', 'car_spaces', \n",
    "    'land_size', 'building_area', 'building_year', 'council', 'latitude', \n",
    "    'longitude', 'region', 'property_count'\n",
    "]\n",
    "\n",
    "# print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value types\n",
    "# print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccfe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify columns that should be integer type\n",
    "numerical_columns = df.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# columns to integers (remove .0) where appropriate\n",
    "for col in numerical_columns:\n",
    "    if df[col].dropna().apply(float.is_integer).all():\n",
    "        df[col] = df[col].astype('Int64')\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09830244",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df.apply(lambda x: x.unique())\n",
    "\n",
    "unique_values_table = pd.DataFrame(unique_values, columns=['Unique Values'])\n",
    "print(unique_values_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2de3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change property type values\n",
    "property_types = {\n",
    "    'h': 'House',\n",
    "    'u': 'Unit',\n",
    "    't': 'Townhouse'\n",
    "}\n",
    "\n",
    "df['property_type'] = df['property_type'].replace(property_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb431f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by 'suburb' and check the uniqueness of 'region' and 'council'\n",
    "inconsistent_suburbs = df.groupby('suburb').agg(\n",
    "    region_unique=('region', 'nunique'),\n",
    "    council_area_unique=('council', 'nunique')  # Fixed the typo here\n",
    ")\n",
    "\n",
    "inconsistent_suburbs = inconsistent_suburbs[\n",
    "    (inconsistent_suburbs['region_unique'] > 1) | (inconsistent_suburbs['council_area_unique'] > 1)\n",
    "]\n",
    "\n",
    "if not inconsistent_suburbs.empty:\n",
    "    print('Inconsistent Suburbs where region or council does not match:')\n",
    "    print(inconsistent_suburbs.index)\n",
    "else:\n",
    "    print(\"All values in 'suburb', 'region', and 'council' match correctly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af23e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing 'region' and 'council' values by using the most frequent value within the same 'suburb'\n",
    "df['region'] = df.groupby('suburb')['region'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Unknown'))\n",
    "df['council'] = df.groupby('suburb')['council'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Unknown'))\n",
    "\n",
    "print(df[['region', 'council']].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82905818",
   "metadata": {},
   "outputs": [],
   "source": [
    "land_size_median = df['land_size'].median()\n",
    "\n",
    "# replace missing values with the median\n",
    "df['land_size'] = df['land_size'].fillna(land_size_median)\n",
    "\n",
    "print(f'Missing values in land_size after filling: {df['land_size'].isna().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46afde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ddfe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_values_count = df.isna().sum()\n",
    "\n",
    "# show only columns with NaN values\n",
    "empty_columns = empty_values_count[empty_values_count > 0]\n",
    "\n",
    "print('Columns with empty values and their count:')\n",
    "print(empty_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6952c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows with empty (NaN) values\n",
    "clean_df = df.dropna()\n",
    "\n",
    "# print(clean_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684587c",
   "metadata": {},
   "source": [
    "# **Assessment Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d595e91-5394-44fd-bc37-fb08c07fc402",
   "metadata": {},
   "source": [
    "## 1. Dropping Null Values and Casting Data Types\n",
    "- Cast the 'price' column in the dataset to an integer type and remove any null values in the 'price' column. \n",
    "- Which of the following statements is true regarding dropping null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c519a7c-a996-4715-8c32-9381332f29cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast 'price' column to integer type, remove null values\n",
    "df.loc[:, 'price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "initial_rows = df.shape[0]\n",
    "df = df.dropna(subset=['price'])\n",
    "final_rows = df.shape[0]\n",
    "rows_deleted = initial_rows - final_rows\n",
    " \n",
    "print(f'Number of rows deleted: {rows_deleted}')\n",
    "print(f'Remaining rows: {final_rows}')\n",
    "print(df[['price']].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bfe412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast 'price' column to integer type, remove null values\n",
    "clean_df.loc[:, 'price'] = pd.to_numeric(clean_df['price'], errors='coerce')\n",
    "initial_rows = clean_df.shape[0]\n",
    "clean_df = clean_df.dropna(subset=['price'])\n",
    "final_rows = clean_df.shape[0]\n",
    "rows_deleted = initial_rows - final_rows\n",
    "\n",
    "print(f'Number of rows deleted: {rows_deleted}')\n",
    "print(f'Remaining rows: {final_rows}')\n",
    "print(clean_df[['price']].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d84b2f0-59b3-4019-bd23-217c5fa21bba",
   "metadata": {},
   "source": [
    "### Answer\n",
    "- **A:** dropping all null values will lead to a loss of 74% of the data\n",
    "- **Correct answer:** dropping null values will lead to a loss of 21% of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e14d7-8a89-463a-9712-8240c4d1a4ee",
   "metadata": {},
   "source": [
    "## 2. Most Common Property Type and Its Percentage\n",
    "What is the most common type of property, and what percentage of the properties does it represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39743c1d-30e5-430f-84a9-26b87257ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts = df['property_type'].value_counts()\n",
    "\n",
    "type_percentage = (type_counts / len(df)) * 100\n",
    "\n",
    "# percentage with two decimal places and the percentage sign\n",
    "type_percentage = type_percentage.apply(lambda x: f'{x:.2f}%')\n",
    "\n",
    "print(type_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d35161",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts = clean_df['property_type'].value_counts()\n",
    "\n",
    "type_percentage = (type_counts / len(clean_df)) * 100\n",
    "\n",
    "# percentage with two decimal places and the percentage sign\n",
    "type_percentage = type_percentage.apply(lambda x: f'{x:.2f}%')\n",
    "\n",
    "print(type_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376751c-5ee0-43ae-91dd-dba87c6594fc",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** The most common type of property is houses, and they represent 67% of the properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a62178-763a-4acc-b08e-78965e2effc9",
   "metadata": {},
   "source": [
    "## 3. Choosing the Right Graph Technique\n",
    "- Create a graph to show the frequency of each property type n descending order. \n",
    "- Which graph technique is the most suitable and straightforward approach for this task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdeebe41-681e-4631-9495-5a76f74d7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts.sort_values(ascending=False).plot(kind='bar', color='lightgreen')\n",
    "\n",
    "plt.title('Property Type Frequencies')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffcd46c-f6d9-4991-8a79-f7975b77756f",
   "metadata": {},
   "source": [
    "### Answer\n",
    "- **A:** Barplot\n",
    "- **Correct answer:** Countplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa86e0d-d79d-40c6-b56f-62a85568b93e",
   "metadata": {},
   "source": [
    "## 4. Room Count vs Median Property Price\n",
    "How does the number of rooms impact the median property price? \n",
    "- Represent this relationship using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091a7f9-752f-448e-a798-4cb4ea977483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by 'rooms' and calculate the median 'price' for each group\n",
    "median_price_per_room = df.groupby('rooms')['price'].median()\n",
    "\n",
    "median_price_per_room = median_price_per_room.astype(int)\n",
    "\n",
    "display(median_price_per_room)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4519ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by 'rooms' and calculate the median 'price' for each group\n",
    "median_price_per_room2 = clean_df.groupby('rooms')['price'].median()\n",
    "\n",
    "median_price_per_room2 = median_price_per_room2.astype(int)\n",
    "\n",
    "display(median_price_per_room2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0b2e7-61c9-4b2f-9675-20160035857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_price_per_room.plot(kind='bar', color='coral')\n",
    "plt.title('Median Property Price by Number of Rooms')\n",
    "plt.xlabel('Number of Rooms')\n",
    "plt.ylabel('Median Property Price')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2550d-5581-48ff-875c-7f60f563fbd3",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** The median property price generally increases with the number of rooms, but there are some exceptions where the price decreases or remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf9cfff-1466-48c2-a43a-969b06e4c02a",
   "metadata": {},
   "source": [
    "## 5. Relationship Between Property Price and Number of Rooms\n",
    "Since the relationship between property price and number of rooms may not be linear, it is appropriate to use a non-parametric correlation coefficient such as Spearman's to determine the strength of the relationship. \n",
    "- Calculate the Spearman correlation coefficient to analyze the relationship between price and number of rooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cec25-a270-4d25-b842-733781475042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Spearman's rank correlation coefficient\n",
    "spearman_corr, p_value = spearmanr(df['rooms'], df['price'])\n",
    "\n",
    "print(f'Correlation coefficient: {spearman_corr}')\n",
    "print(f'P_value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d813643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Spearman's rank correlation coefficient\n",
    "spearman_corr2, p_value2 = spearmanr(clean_df['rooms'], clean_df['price'])\n",
    "\n",
    "print(f'Correlation coefficient: {spearman_corr2}')\n",
    "print(f'P_value: {p_value2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd66ed8-8855-4531-a428-937b6f3588e6",
   "metadata": {},
   "source": [
    "#### **Interpretation**\n",
    "\n",
    "#### Spearman's Rank Correlation Coefficient: **0.5043**\n",
    "The value of **0.5043** indicates a **moderate positive monotonic relationship** between the number of rooms and the property price. This suggests that, generally, as the number of rooms increases, the property price also tends to increase. However, the relationship is not perfectly linear, meaning there are variations and the correlation is moderate.\n",
    "\n",
    "#### P-value: **0.0**\n",
    "The **p-value of 0.0** is extremely small, indicating that the correlation is **statistically significant**. This means there is a very low probability that the observed correlation is due to random chance. Therefore, we can confidently conclude that the relationship between the number of rooms and property price is meaningful and not coincidental.\n",
    "\n",
    "#### Conclusion:\n",
    "The **moderate positive Spearman correlation (0.5043)** suggests that as the number of rooms increases, property prices tend to rise, although not in a perfectly linear manner. The **strong statistical significance (p-value = 0.0)** further supports the conclusion that this relationship is genuine and unlikely to be due to random variation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e150b30-7745-44a0-a6b3-3775424323ad",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** The correlation between property price and number of rooms is moderate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb98a4f-0d6a-4e40-afce-f2da57bd96d8",
   "metadata": {},
   "source": [
    "## 6. CBD Distance and Property Price\n",
    "Is there a relationship between the distance from the Central Business District (CBD) and the property price?\n",
    "- Use a scatter plot to visualize the relationship and calculate a correlation coefficient to determine the strength and direction of the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0cf2b7-f279-4bb8-b31c-31daf0306b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill empty values with mean\n",
    "df['distance'] = df['distance'].fillna(df['distance'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c19051-ffbc-4453-bbf1-55fbb3fdba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['distance'], df['price'], color='limegreen')\n",
    "plt.title('Property Price vs Distance from CBD')\n",
    "plt.xlabel('distance')\n",
    "plt.ylabel('Property Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pearson correlation coefficient\n",
    "pearson_corr, p_value = pearsonr(df['distance'], df['price'])\n",
    "\n",
    "print(f'Pearson correlation coefficient: {pearson_corr}')\n",
    "print(f'P-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a80fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(clean_df['distance'], clean_df['price'], color='limegreen')\n",
    "plt.title('Property Price vs Distance from CBD')\n",
    "plt.xlabel('distance')\n",
    "plt.ylabel('Property Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pearson correlation coefficient\n",
    "pearson_corr2, p_value2 = pearsonr(clean_df['distance'], clean_df['price'])\n",
    "\n",
    "print(f'Pearson correlation coefficient: {pearson_corr2}')\n",
    "print(f'P-value: {p_value2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9598f90e-5ff8-45a4-be20-912d67e591a6",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** There is a weak negative correlation between the distance from CBD and the property price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b41e759-ccbd-4f09-8729-afe47e0bfe3e",
   "metadata": {},
   "source": [
    "## 7. Property Size vs Property Price\n",
    "Is there a relationship between the property size and the property price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2d299-4611-4309-a378-f8f786c85aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot to visualize the relationship\n",
    "plt.scatter(df['land_size'], df['price'], color='hotpink')\n",
    "plt.title('Property Price vs Property Size')\n",
    "plt.xlabel('Size (land and building)')\n",
    "plt.ylabel('price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pearson correlation coefficient\n",
    "pearson_corr, p_value = pearsonr(df['land_size'], df['price'])\n",
    "\n",
    "print(f'Pearson correlation coefficient: {pearson_corr}')\n",
    "print(f'P-value: {p_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot to visualize the relationship\n",
    "plt.scatter(clean_df['land_size'], clean_df['price'], color='hotpink')\n",
    "plt.title('Property Price vs Property Size')\n",
    "plt.xlabel('Size (land and building)')\n",
    "plt.ylabel('price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Pearson correlation coefficient\n",
    "pearson_corr2, p_value2 = pearsonr(clean_df['land_size'], clean_df['price'])\n",
    "\n",
    "print(f'Pearson correlation coefficient: {pearson_corr2}')\n",
    "print(f'P-value: {p_value2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bfc6b-cfe3-4996-9ad4-2fd9729bbbe8",
   "metadata": {},
   "source": [
    "- **A:** There is a weak positive linear relationship between property size and property price.\n",
    "- **Correct answer:** There is a weak positive correlation between the two variables, but the relationship might not be linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e216e9-a14a-4ef7-8f33-6f70bac5742b",
   "metadata": {},
   "source": [
    "## 8. Land Size and Property Price: Quantile-Based Discretization and Bar Plot Comparison\n",
    "When looking visually at the relationship between land size and property price, a scatter plot might not be the most informative way to visualize this relationship.\n",
    "- Use a **quantile-based discretization function** to categorize the Landsize variable and create a bar plot.\n",
    "- When using **q=7**, how does the relationship between land size and median property price change compared to using a lower quantile value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a014afc-b877-4aa3-a7d4-0a39f1254369",
   "metadata": {},
   "source": [
    "When using a quantile-based discretization function to categorize the Landsize variable into quantiles (such as q=7), you're dividing the land size into 7 equal-sized groups, each containing approximately the same number of data points. This approach transforms a continuous variable (land size) into a categorical one, where each group corresponds to a range of land sizes. By comparing this method to a lower quantile value (such as q=4), you can observe how the relationship between land size and median property price changes.\n",
    "\n",
    "#### Summary:\n",
    "- Using **q=7** provides a higher level of detail, allowing you to more clearly observe subtle differences in property price across land sizes.\n",
    "- Using a lower quantile value (e.g., **q=4**) smooths out these differences, resulting in a less granular view of the relationship, which could mask finer distinctions in property prices as they relate to land size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188c12c-b5b2-4355-b31f-9f6eacb8a600",
   "metadata": {},
   "source": [
    "### Answer\n",
    "- **A:** The relationship between land size and property price becomes more pronounced when using q=7 compared to a lower quantile value.\n",
    "- **Correct answer:** The frequency of properties with a larger land size having a lower price than properties with a smaller land size increases when using q=7, compared to a lower quantile value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9391dd-927f-4801-b634-f33756188bf7",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "Using **q=7** means dividing the land size into 7 quantiles, which creates more granular groups, allowing for finer distinctions in the relationship between land size and property price. This increases the resolution of the data, helping to reveal more subtle trends and variations.\n",
    "\n",
    "In contrast, using a lower quantile value (e.g., **q=4**) results in fewer groups, which may smooth out the relationship and make it harder to see more pronounced differences.\n",
    "\n",
    "Thus, when using **q=7**, the relationship between land size and property price is more pronounced due to the finer categorization, making it easier to observe variations across different land size ranges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ad0f2-2df4-419b-b61e-edd61c1aaa15",
   "metadata": {},
   "source": [
    "## 9. Identifying Outliers in Property Prices: Box Plot and Tukey's Method\n",
    "Does the dataset contain any properties that are priced significantly above or below the average? \n",
    "- Create a box plot to detect potential outliers, and u\n",
    "- Use Tukey's method with a 1.5 threshold to create a new dataframe without these outliers. \n",
    "- What percentage of properties are identified as outliers using Tukey's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71919182-404e-4d0d-b1dd-9754f8de2e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot to visually identify potential outliers\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(df['price'])\n",
    "plt.title('Property Prices')\n",
    "plt.ylabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c4967-7391-4bc3-84e6-b3b0c807591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresholds\n",
    "Q1 = df['price'].quantile(0.25)\n",
    "Q3 = df['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Tukey's method to detect outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# identify outliers\n",
    "outliers = df[(df['price'] < lower_bound) | (df['price'] > upper_bound)]\n",
    "\n",
    "print(f'Lower bound: {lower_bound}')\n",
    "print(f'Upper bound: {upper_bound}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064d7d0-b20c-47d1-8abf-0f4bf1701763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df without outliers\n",
    "outlier_free_df = df[(df['price'] >= lower_bound) & (df['price'] <= upper_bound)]\n",
    "\n",
    "# percentage of outliers\n",
    "percentage_outliers = (len(outliers) / len(df)) * 100\n",
    "\n",
    "print(f'Percentage of outliers: {percentage_outliers:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561476f-9228-469c-9ed2-076eb0fbf6a1",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** 4.69%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5f8f2f-f2b0-4015-9739-8040e0ca2ec5",
   "metadata": {},
   "source": [
    "## 10.  Property Price Distribution: QQ Plot, Skewness, and Kurtosis Calculation\n",
    "How is the property price distribution? \n",
    "- Create a QQ (Quantile-Quantile) Plot to compare the distribution of property prices to a normal distribution visually\n",
    "- Calculate the skew and curtosis to get a better understanding of the shape of the distribution analytically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e982c-c479-47ca-8cb4-31ae720ebec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create QQ plot to compare distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "stats.probplot(df['price'], dist='norm', plot=plt)\n",
    "plt.title('QQ Plot: Property Prices vs Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1101fc0f-8da5-42c8-8b68-3d92644a2743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate skewness and kurtosis\n",
    "skewness = df['price'].skew()\n",
    "kurtosis = df['price'].kurtosis()\n",
    "\n",
    "print(f'Skewness: {skewness:.2f}')\n",
    "print(f'Kurtosis: {kurtosis:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65022b3-d0e0-464b-a5c5-44318c8119eb",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** The property price distribution is positively skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336303a-cebf-4b97-a361-afcf50a0c400",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "**Skewness of 2.59** indicates a **positive skew**, meaning the distribution is **right-skewed** (with a long tail on the right side), where most property prices are lower, but a few high-priced properties create a longer tail on the right.\n",
    "\n",
    "The **kurtosis value of 13.10** confirms the presence of **heavy tails** (more extreme high values than normal), which further supports the idea of **outliers** in the higher price range, a characteristic of a positively skewed distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094e0e4-df96-4f90-b873-17a10e9e6f34",
   "metadata": {},
   "source": [
    "## 11. Property Price Distribution: Kolmogorov-Smirnov Test\n",
    "What is the outcome of performing a normality test, such as the *Kolmogorov-Smirnov Test*, on the distribution of property prices? Specifically, what possible conclusions can be drawn from the test results regarding the similarity of the distribution to a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c538f9-4652-4908-bd85-af0812974cc5",
   "metadata": {},
   "source": [
    "**Kolmogorov-Smirnov Test:**\n",
    "- Null Hypothesis (H₀): The data follows a normal distribution.\n",
    "- Alternative Hypothesis (H₁): The data does not follow a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce719d1-2a6e-4d94-9736-b1435c049e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize prices (mean 0, std 1)\n",
    "standardized_prices = (df['price'] - df['price'].mean()) / df['price'].std()\n",
    "# print(standardized_prices)\n",
    "\n",
    "# Kolmogorov-Smirnov test\n",
    "ks_statistic, p_value = stats.kstest(standardized_prices, 'norm')\n",
    "\n",
    "print(f'KS statistic: {ks_statistic:.4f}')\n",
    "print(f'p-value: {p_value:.4f}')\n",
    "\n",
    "# interpretation based on p-value\n",
    "if p_value < 0.05:\n",
    "    print('The null hypothesis is rejected: The distribution of property prices is significantly different from a normal distribution.')\n",
    "else:\n",
    "    print('The null hypothesis cannot be rejected: The distribution of property prices is not significantly different from a normal distribution.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a137d-7938-40ca-b25a-7056b7263163",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** The test results indicate that the distribution of property prices in the Melbourne housing market dataset is significantly different from a normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6a43b-4602-41f9-b1f9-d701a120ad25",
   "metadata": {},
   "source": [
    "## 12. Property Prices Across Different Property Types: Box Plot Visualization\n",
    "How do property prices differ between different property types? \n",
    "- Create a box plot to show the distribution of property prices for each property type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cc026-fc1a-47be-bffa-203ea790d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot to show the distribution of property prices for each property type\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='property_type', y='price', data=df)\n",
    "\n",
    "plt.title('Property Price Distribution by Property Type', fontsize=16)\n",
    "plt.xlabel('Property Type', fontsize=14)\n",
    "plt.ylabel('Property Price', fontsize=14)\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45282e14-9447-48bb-89cd-54184f3458b1",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** Houses have higher property prices than units and townhouses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b334f06-5b4c-454e-9627-77828020f37f",
   "metadata": {},
   "source": [
    "## 13. Median Property Prices by Region: Identifying the Highest and Lowest Median Prices\n",
    "- What is the region with the highest median price? \n",
    "- Conversely, which region has the lowest median price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a0d4f-97e6-4856-a463-cfe6f79f74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# median of 'price' by 'region'\n",
    "region_medians = df.groupby('region')['price'].median()\n",
    "\n",
    "highest_median_region = region_medians.idxmax()\n",
    "highest_median_price = int(region_medians.max())\n",
    "\n",
    "lowest_median_region = region_medians.idxmin()\n",
    "lowest_median_price = int(region_medians.min())\n",
    "\n",
    "print(f'Region with the highest median price: {highest_median_region} (Median Price: {highest_median_price})')\n",
    "print(f'Region with the lowest median price: {lowest_median_region} (Median Price: {lowest_median_price})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bb260c-2b39-4524-989f-b347f2fafb95",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** -Most expensive: Southern Metropolitan, Cheapest: Western Victoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041efc95-fc82-419a-b26a-fffede51e644",
   "metadata": {},
   "source": [
    "## 14. Region and Property Price: Chi-Square Test and Cramer's V Coefficient Analysis\n",
    "Is there a statistically significant association between the region and the price of properties? \n",
    "- Use the *chi square test* to assess the independence between region and price\n",
    "- Use *Cramer's V coefficient* to quantify the strength of the association between these two features\n",
    "- Divide the price variable into five distinc ranges.\n",
    " \n",
    "*It's worth noting that in order to perform this analysis, both features need to be categorical.*\n",
    "*Use Cohen (1988) interpretation on Cramer's V, which depends on the degrees of freedom.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12f156-77ff-4d45-b444-940d85ad35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the 'price' column into five distinct ranges\n",
    "price_bins = [0, 300000, 500000, 700000, 900000, np.inf]\n",
    "price_labels = ['0-300k', '300k-500k', '500k-700k', '700k-900k', '900k+']\n",
    "df['price_category'] = pd.cut(df['price'], bins=price_bins, labels=price_labels)\n",
    "\n",
    "# create a contingency table of 'price_category' vs 'region'\n",
    "contingency_table = pd.crosstab(df['price_category'], df['region'])\n",
    "\n",
    "# Chi-Square Test of Independence\n",
    "chi2_stat, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "# Cramer's V coefficient\n",
    "n = contingency_table.sum().sum()\n",
    "min_dim = min(contingency_table.shape)\n",
    "cramers_v = np.sqrt(chi2_stat / (n * min_dim))\n",
    "\n",
    "print(f'Chi-Square Test Statistic: {chi2_stat:.4f}')\n",
    "print(f'p-value: {p_val:.4f}')\n",
    "print(f\"Cramer's V: {cramers_v:.4f}\")\n",
    "\n",
    "# Chi-Square Test Interpretation\n",
    "if p_val < 0.05:\n",
    "    print('There is a statistically significant association between region and price (reject H0).')\n",
    "else:\n",
    "    print('There is no statistically significant association between region and price (fail to reject H0).')\n",
    "\n",
    "# Cramer's V Interpretation based on Cohen (1988)\n",
    "if cramers_v < 0.1:\n",
    "    print('The association between region and price is weak.')\n",
    "elif cramers_v < 0.3:\n",
    "    print('The association between region and price is moderate.')\n",
    "else:\n",
    "    print('The association between region and price is strong.')\n",
    "\n",
    "# contingency table\n",
    "sns.heatmap(contingency_table, annot=True, cmap='Blues', fmt='d')\n",
    "plt.title('Contingency Table Heatmap')\n",
    "plt.xticks(rotation=55)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082354e9-eb1f-4159-8dd8-d975a7972e7a",
   "metadata": {},
   "source": [
    "### Answer\n",
    "**A:** Yes, there is a significant relationship between the region and the price of properties, and the relationship is medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeeb94d-a8d1-49c2-ba3b-d350ac1572aa",
   "metadata": {},
   "source": [
    "## 15. Percentage Change in Median Property Price between the Earliest and Latest Year\n",
    " How much was the percentage increase or decrease in *median price* of properties sold between the earliest year and the latest year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double check results\n",
    "\n",
    "# 1. Split rows between years (found in the date column), drop the ones where there's no year information.\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Handle invalid dates as NaT\n",
    "\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "# drop rows where 'year' is NaN\n",
    "df = df.dropna(subset=['year'])\n",
    "\n",
    "# year column to integer for grouping\n",
    "df['year'] = df['year'].astype(int)\n",
    "\n",
    "# group by year\n",
    "years_dict = {year: group.reset_index(drop=True) for year, group in df.groupby('year')}\n",
    "\n",
    "print(f'DataFrames split by years: {list(years_dict.keys())}')\n",
    "\n",
    "\n",
    "# 2. check the median price for each year's df and return the result (dyear, number of sales (= number of rows), and the median price fore ach year\n",
    "\n",
    "def calculate_yearly_median(dataframe):\n",
    "    dataframe['date'] = pd.to_datetime(dataframe['date'], errors='coerce') # check date format\n",
    "    dataframe['year'] = dataframe['date'].dt.year # extract year\n",
    "    \n",
    "    # drop rows where 'year' or 'price' is missing\n",
    "    dataframe = dataframe.dropna(subset=['year', 'price'])\n",
    "    \n",
    "    # year to integer and price to numeric\n",
    "    dataframe['year'] = dataframe['year'].astype(int)\n",
    "    dataframe['price'] = pd.to_numeric(dataframe['price'], errors='coerce')\n",
    "    \n",
    "    # group by year, calculate sales and median price\n",
    "    result = dataframe.groupby('year').agg(\n",
    "        number_of_sales=('price', 'count'),\n",
    "        median_price=('price', 'median')\n",
    "    ).reset_index()\n",
    "    \n",
    "    return result\n",
    "\n",
    "yearly_stats = calculate_yearly_median(df)\n",
    "print(yearly_stats)\n",
    "\n",
    "print('')\n",
    "\n",
    "# 3.  Calculate the percentage difference in median prices between first and last year\n",
    "def calculate_difference(yearly_stats, year1, year2):\n",
    "    price1 = yearly_stats.loc[yearly_stats['year'] == year1, 'median_price'].values[0]\n",
    "    price2 = yearly_stats.loc[yearly_stats['year'] == year2, 'median_price'].values[0]\n",
    "    \n",
    "    percentage_diff = ((price2 - price1) / price1) * 100\n",
    "    return percentage_diff\n",
    "\n",
    "difference_2016_2018 = calculate_difference(yearly_stats, 2016, 2018)\n",
    "print(f'The percentage difference between 2016 and 2018 in the median sales price is {difference_2016_2018:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56c412e-d7bd-4214-bb7d-02c8eb7bb11f",
   "metadata": {},
   "source": [
    "### Answer\n",
    "- **A:** -10.37%\n",
    "- **Correct answer:** -5.48%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f085e-283a-4c55-a350-0eeed54b254c",
   "metadata": {},
   "source": [
    "## 16. Identifying the Month with the Highest Property Sales Volume\n",
    "Is there a specific month during which more houses are sold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5520ae2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-check sales per month again\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
    "\n",
    "# Extract the month name from the Date\n",
    "df['month'] = df['date'].dt.month_name()\n",
    "\n",
    "# Group by month and count rows\n",
    "rows_per_month = df['month'].value_counts().sort_index()\n",
    "\n",
    "# results to df\n",
    "result = result.sort_values(by='row_count', ascending=False)\n",
    "result.columns = ['month', 'row_count']\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert 'date' to datetime format\n",
    "# df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Ensure Date is in datetime format\n",
    "\n",
    "# # Extract the month from the 'date' column\n",
    "# df['month'] = df['date'].dt.month_name()  # Get full month names (e.g., 'January', 'February')\n",
    "\n",
    "# # Count the number of properties sold per month\n",
    "# sales_per_month = df['month'].value_counts().sort_index()  # Count occurrences per month, sorted by month\n",
    "\n",
    "# # Calculate the average sales per month\n",
    "# average_sales_per_month = sales_per_month.mean()\n",
    "\n",
    "# # Display the average sales per month\n",
    "# print(f\"Average sales per month: {average_sales_per_month:.2f}\")\n",
    "\n",
    "# # Month with the most sales\n",
    "# most_sales_month = sales_per_month.idxmax()\n",
    "# most_sales_count = sales_per_month.max()\n",
    "\n",
    "# # Display the month with the most sales\n",
    "# print(f\"The month with the most houses sold is {most_sales_month} with {most_sales_count} sales.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392ab8c-2781-482a-a6f6-4a447a56a873",
   "metadata": {},
   "source": [
    "### Answer\n",
    "- **A:** March.\n",
    "- **Correct answer:** November."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
